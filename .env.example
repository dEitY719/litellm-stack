# ═══════════════════════════════════════════════════
# 환경 설정 (Required)
# ═══════════════════════════════════════════════════
#
# LITELLM_ENV: 실행 환경 선택
# - home:     개인 PC (로컬 개발, SSL 검증 활성화)
# - external: 회사 외부 PC (공개 GitHub, 네트워크 사용 가능)
# - internal: 회사 내부 PC (프록시, SSL 검증 비활성화, CA 인증서 필수)
#
LITELLM_ENV=home

# ═══════════════════════════════════════════════════
# External LLM API Keys (Optional)
# ═══════════════════════════════════════════════════

# Gemini API Key
# Get it from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-api-key-here

# OpenAI API Key (Optional, for routing to OpenAI)
# OPENAI_API_KEY=your-openai-api-key-here


# ═══════════════════════════════════════════════════
# GPU Configuration (Advanced - Optional)
# ═══════════════════════════════════════════════════
# Default values are set in docker-compose.yml
# Only uncomment and adjust if needed for your specific hardware

# GPU Memory Overhead (bytes)
# 1GB = 1073741824, 2GB = 2147483648
# Increase for multiple simultaneous models, decrease for single model
# OLLAMA_GPU_OVERHEAD=1073741824

# GPU Layer Offloading (auto-detect by default)
# Set manually if you know your model's layer count
# Example: tinyllama=25 layers, gpt-oss:20b=40 layers
# OLLAMA_NUM_GPU=25

# Parallel Processing Count
# Number of concurrent requests to handle
# OLLAMA_NUM_PARALLEL=2

# Maximum Loaded Models
# How many models to keep in GPU memory simultaneously
# OLLAMA_MAX_LOADED_MODELS=2
