# ═══════════════════════════════════════════════════
# External LLM API Keys (Optional)
# ═══════════════════════════════════════════════════

# Gemini API Key
# Get it from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-api-key-here

# OpenAI API Key (Optional, for routing to OpenAI)
# OPENAI_API_KEY=your-openai-api-key-here


# ═══════════════════════════════════════════════════
# GPU Configuration (Advanced - Optional)
# ═══════════════════════════════════════════════════
# Default values are set in docker-compose.yml
# Only uncomment and adjust if needed for your specific hardware

# GPU Memory Overhead (bytes)
# 1GB = 1073741824, 2GB = 2147483648
# Increase for multiple simultaneous models, decrease for single model
# OLLAMA_GPU_OVERHEAD=1073741824

# GPU Layer Offloading (auto-detect by default)
# Set manually if you know your model's layer count
# Example: tinyllama=25 layers, gpt-oss:20b=40 layers
# OLLAMA_NUM_GPU=25

# Parallel Processing Count
# Number of concurrent requests to handle
# OLLAMA_NUM_PARALLEL=2

# Maximum Loaded Models
# How many models to keep in GPU memory simultaneously
# OLLAMA_MAX_LOADED_MODELS=2
