services:
  # ═══════════════════════════════════════════════════
  # Ollama: 로컬 LLM 추론 엔진 (통합)
  # ═══════════════════════════════════════════════════
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # ═══════════════════════════════════════════════════
      # 메모리 관리
      # ═══════════════════════════════════════════════════

      # 모델 자동 언로드 시간 (메모리 관리)
      OLLAMA_KEEP_ALIVE: "5m"

      # ═══════════════════════════════════════════════════
      # GPU 최적화
      # ═══════════════════════════════════════════════════

      # GPU 오버헤드 (시스템 예약 메모리)
      # 값: 바이트 단위 (1GB = 1073741824)
      # 권장: RTX 5070 Ti (16GB)의 경우 1-2GB
      OLLAMA_GPU_OVERHEAD: "1073741824"  # 1GB

      # GPU 병렬 처리 수 (동시 요청)
      # 값: 1-8 (높을수록 동시성 증가, 메모리 사용 증가)
      # 권장: 단일 사용자 환경 1-2, 멀티유저 4-8
      OLLAMA_NUM_PARALLEL: "2"

      # 메모리에 유지할 최대 모델 수
      # 값: 1-5 (VRAM 용량에 따라 조정)
      # 권장: 16GB VRAM의 경우 2-3개
      OLLAMA_MAX_LOADED_MODELS: "2"

      # Flash Attention 활성화 (성능 향상)
      # 값: 1 (활성화), 0 (비활성화)
      # 참고: Ampere 이상 GPU에서 지원 (RTX 30xx, 40xx, 50xx)
      OLLAMA_FLASH_ATTENTION: "1"

      # GPU 레이어 오프로드 수 (CUDA 병렬화)
      # 값: 자동 감지 또는 명시적 수 (0-128)
      # 권장: 25 (RTX 5070 Ti 기본값)
      OLLAMA_NUM_GPU: "25"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ═══════════════════════════════════════════════════
  # PostgreSQL: LiteLLM 설정 및 로그 저장소
  # ═══════════════════════════════════════════════════
  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    ports:
      - "5431:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10

  # ═══════════════════════════════════════════════════
  # LiteLLM: AI Gateway (Proxy)
  # ═══════════════════════════════════════════════════
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-v1.73.0-stable
    restart: unless-stopped
    env_file: .env
    volumes:
      - ./litellm_settings.yml:/app/config.yml
    command:
      - "--config=config.yml"
    ports:
      - "4444:4000"
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
      STORE_MODEL_IN_DB: "True"
      LITELLM_MASTER_KEY: "sk-4444"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  postgres_data:
    name: litellm_postgres_data
  ollama_data:
    name: litellm_ollama_data

networks:
  default:
    name: litellm-network  
