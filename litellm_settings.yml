model_list:
  # ═══════════════════════════════════════════════════
  # Ollama 로컬 모델 (단일 인스턴스)
  # ═══════════════════════════════════════════════════

  # 저사양 PC: 항상 사용 가능 (~50MB)
  - model_name: tinyllama
    litellm_params:
      model: ollama/tinyllama
      api_base: http://ollama:11434
    model_info:
      mode: chat
      supports_function_calling: false
      max_tokens: 2048

  # 고사양 PC: 선택적 사용 (~11GB VRAM)
  - model_name: gpt-oss-20b
    litellm_params:
      model: ollama/gpt-oss:20b
      api_base: http://ollama:11434
    model_info:
      mode: chat
      supports_function_calling: true
      max_tokens: 8192

  # 임베딩 모델 (~2GB)
  - model_name: bge-m3
    litellm_params:
      model: ollama/bge-m3:latest
      api_base: http://ollama:11434
    model_info:
      mode: embedding
      max_input_tokens: 8192

  # ═══════════════════════════════════════════════════
  # 외부 API 모델
  # ═══════════════════════════════════════════════════

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: gemini/gemini-2.5-flash-lite
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY

general:
  debug: true
  litellm_settings:
    success_callback: ["postgres"]
    failure_callback: ["postgres"]

# 1. UI 로그 저장을 위한 설정 (Admin 페이지 가이드 내용)
general_settings:
  store_model_in_db: true
  store_prompts_in_spend_logs: true

# 2. (선택사항) 터미널에서도 보고 싶다면 유지, 아니면 삭제해도 됨
litellm_settings:
  log_payload: true
  ssl_verify: false  # 개발 환경: SSL 인증서 검증 비활성화
