#!/bin/bash
# setup_models.sh - PC ì‚¬ì–‘ì— ë”°ë¥¸ ìë™ ëª¨ë¸ ì„¤ì • (Idempotent)

set -e

echo "=================================="
echo "LiteLLM ëª¨ë¸ ìë™ ì„¤ì •"
echo "=================================="
echo ""

# VRAM í™•ì¸ (nvidia-smi í•„ìš”)
if command -v nvidia-smi &> /dev/null; then
    VRAM_MB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    VRAM_GB=$((VRAM_MB / 1024))
    echo "âœ“ GPU ê°ì§€: ${VRAM_GB}GB VRAM"
else
    VRAM_GB=0
    echo "âš  GPU ë¯¸ê°ì§€ (CPU ëª¨ë“œ)"
fi

echo ""

# Compose ìŠ¤íƒ ì‹œì‘
echo "[1/3] Docker Compose ìŠ¤íƒ ì‹œì‘ ì¤‘..."
docker compose up -d
sleep 5

# ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸)
echo ""
echo "[2/3] ëª¨ë¸ ì„¤ì • ì¤‘..."
echo ""

INSTALLED_MODELS=$(docker exec ollama ollama list 2>/dev/null | awk 'NR>1 {print $1}' | sed 's/:latest//' || echo "")

# ì„¤ì •í•  ëª¨ë¸ ëª©ë¡ ê²°ì •
MODELS_TO_INSTALL=()
MODELS_INSTALLED=()

# ê¸°ë³¸ ëª¨ë¸ (í•­ìƒ)
if echo "$INSTALLED_MODELS" | grep -q "^tinyllama"; then
    echo "  âœ… tinyllama (ì´ë¯¸ ì„¤ì¹˜ë¨, ~50MB)"
    MODELS_INSTALLED+=("tinyllama")
else
    echo "  ğŸ“¥ tinyllama ì„¤ì¹˜ ì¤‘ (~50MB)..."
    docker exec ollama ollama pull tinyllama > /dev/null 2>&1
    MODELS_INSTALLED+=("tinyllama")
fi

# ì‚¬ì–‘ë³„ ëª¨ë¸
if [ "$VRAM_GB" -ge 14 ]; then
    echo ""
    echo "  âœ“ ê³ ì‚¬ì–‘ PC ê°ì§€ (${VRAM_GB}GB VRAM)"
    echo ""

    # gpt-oss:20b
    if echo "$INSTALLED_MODELS" | grep -q "^gpt-oss"; then
        echo "  âœ… gpt-oss:20b (ì´ë¯¸ ì„¤ì¹˜ë¨, ~11GB)"
        MODELS_INSTALLED+=("gpt-oss-20b")
    else
        echo "  ğŸ“¥ gpt-oss:20b ì„¤ì¹˜ ì¤‘ (~11GB, ì•½ 10ë¶„ ì†Œìš”)..."
        docker exec ollama ollama pull gpt-oss:20b > /dev/null 2>&1
        MODELS_INSTALLED+=("gpt-oss-20b")
    fi

    # bge-m3
    if echo "$INSTALLED_MODELS" | grep -q "^bge-m3"; then
        echo "  âœ… bge-m3 (ì´ë¯¸ ì„¤ì¹˜ë¨, ~2GB)"
        MODELS_INSTALLED+=("bge-m3")
    else
        echo "  ğŸ“¥ bge-m3 ì„¤ì¹˜ ì¤‘ (~2GB)..."
        docker exec ollama ollama pull bge-m3:latest > /dev/null 2>&1
        MODELS_INSTALLED+=("bge-m3")
    fi

    # gpt-oss:20b ì‚¬ì „ ë¡œë“œ (ìµœì†Œ 5ì´ˆ ì´ìƒ ì‹¤í–‰í•´ì„œ ë©”ëª¨ë¦¬ì— ë¡œë“œ)
    echo "  ğŸ”„ gpt-oss:20b ì‚¬ì „ ë¡œë“œ ì¤‘..."
    timeout 30 docker exec ollama ollama run gpt-oss:20b "warmup" > /dev/null 2>&1 || true

    MODELS="tinyllama, gpt-oss-20b, bge-m3"
else
    echo ""
    echo "  âœ“ ì €ì‚¬ì–‘ PC ê°ì§€ (${VRAM_GB}GB VRAM)"
    echo "  âš ï¸  gpt-oss:20bëŠ” ìƒëµí•©ë‹ˆë‹¤ (14GB+ VRAM ê¶Œì¥)"
    echo ""

    MODELS="tinyllama"
fi

# í…ŒìŠ¤íŠ¸
echo "[3/3] ì„¤ì • í™•ì¸ ì¤‘..."
echo ""

# LiteLLM í—¬ìŠ¤ ì²´í¬
if curl -f http://localhost:4444/health/liveliness > /dev/null 2>&1; then
    echo "  âœ… LiteLLM í”„ë¡ì‹œ ì •ìƒ"
else
    echo "  âŒ LiteLLM í”„ë¡ì‹œ ì‘ë‹µ ì—†ìŒ"
    exit 1
fi

# ì„¤ì •ëœ ëª¨ë¸ ëª©ë¡ í‘œì‹œ
echo "  âœ… ì„¤ì •ëœ ëª¨ë¸: ${MODELS}"

echo ""
echo "=================================="
echo "ì„¤ì • ì™„ë£Œ! (Idempotent - ì•ˆì „í•˜ê²Œ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ ê°€ëŠ¥)"
echo "=================================="
echo ""
echo "ğŸ“ ì„¤ëª…:"
echo "  â€¢ âœ… = ì´ë¯¸ ì„¤ì¹˜ë¨ (ë‹¤ì‹œ ì‹¤í–‰í•´ë„ skip)"
echo "  â€¢ ğŸ“¥ = ìƒˆë¡œ ì„¤ì¹˜ (ì²« ì‹¤í–‰ ì‹œì—ë§Œ)"
echo ""
echo "ğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:"
echo ""
echo "  curl http://localhost:4444/v1/chat/completions \\"
echo "    -H \"Authorization: Bearer sk-4444\" \\"
echo "    -H \"Content-Type: application/json\" \\"
echo "    -d '{"
echo "      \"model\": \"tinyllama\","
echo "      \"messages\": [{\"role\": \"user\", \"content\": \"ì•ˆë…•?\"}]"
echo "    }'"
echo ""
