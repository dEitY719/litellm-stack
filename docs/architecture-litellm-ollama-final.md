# LiteLLM + Ollama ìµœì¢… ì•„í‚¤í…ì²˜ ì„¤ê³„

**ì‘ì„±ì¼**: 2025-12-08
**ë²„ì „**: Final v1.0
**ê²°ì •**: Option C (í•˜ì´ë¸Œë¦¬ë“œ) - ë‹¨ì¼ Compose ìŠ¤íƒ, í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬, ë„¤íŠ¸ì›Œí¬ ê³µìœ 

---

## ğŸ“‹ ëª©ì°¨

1. [í•µì‹¬ ì„¤ê³„ ê²°ì •](#1-í•µì‹¬-ì„¤ê³„-ê²°ì •)
2. [ìµœì¢… ì•„í‚¤í…ì²˜](#2-ìµœì¢…-ì•„í‚¤í…ì²˜)
3. [êµ¬í˜„: ë‹¨ì¼ Compose ìŠ¤íƒ](#3-êµ¬í˜„-ë‹¨ì¼-compose-ìŠ¤íƒ)
4. [ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ëŒ€ì‘](#4-ì €ì‚¬ì–‘ê³ ì‚¬ì–‘-pc-ëŒ€ì‘)
5. [SOLID ì›ì¹™ ì¤€ìˆ˜ í™•ì¸](#5-solid-ì›ì¹™-ì¤€ìˆ˜-í™•ì¸)
6. [ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](#6-ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ)
7. [ìš´ì˜ ê°€ì´ë“œ](#7-ìš´ì˜-ê°€ì´ë“œ)
8. [ë¬¸ì œ í•´ê²°](#8-ë¬¸ì œ-í•´ê²°)

---

## 1. í•µì‹¬ ì„¤ê³„ ê²°ì •

### 1.1 ìµœì¢… ì„ íƒ: ë‹¨ì¼ Compose ìŠ¤íƒ + í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬

```
âœ… ì±„íƒ: í•œ docker-compose.ymlì— ë¬¶ì–´ ë„¤íŠ¸ì›Œí¬ë§Œ ê³µìœ 
âŒ ê¸°ê°: ë‹¨ì¼ ì»¨í…Œì´ë„ˆë¡œ í†µí•© (SOLID ìœ„ë°°)
âŒ ê¸°ê°: ì™„ì „ ë¶„ë¦¬ (ê´€ë¦¬ ë³µì¡ë„ ì¦ê°€)
```

**ì´ìœ :**

| ê´€ì  | í‰ê°€ |
|------|------|
| **SOLID ì›ì¹™** | âœ… SRP ì¤€ìˆ˜ (í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬: Ollama vs LiteLLM) |
| **ê´€ë¦¬ ë³µì¡ë„** | âœ… ë‹¨ì¼ ëª…ë ¹ì–´ (`docker compose up`) |
| **ë„¤íŠ¸ì›Œí¬** | âœ… ì„œë¹„ìŠ¤ëª…ìœ¼ë¡œ ì§ì ‘ í†µì‹  (`http://ollama:11434`) |
| **í™•ì¥ì„±** | âœ… ëª¨ë¸ ì¶”ê°€ ì‹œ ì„¤ì •ë§Œ ë³€ê²½ |
| **ì´ì‹ì„±** | âœ… ë‹¨ì¼ íŒŒì¼ë¡œ ì „ì²´ ìŠ¤íƒ ì¬í˜„ ê°€ëŠ¥ |

### 1.2 ì£¼ìš” ë³€ê²½ ì‚¬í•­

#### Before (ë¶„ë¦¬ëœ í”„ë¡œì íŠ¸)

```text
devnet_env_setup/
â””â”€ docker-compose.yml
   â””â”€ ollama (11434) â†’ gpt-oss:20b, bge-m3

litellm/
â””â”€ docker-compose.yml
   â”œâ”€ tinyllama1 (11431) â†’ tinyllama
   â”œâ”€ litellm (4444)
   â””â”€ litellm_db (5432)
```

#### After (í†µí•© Compose ìŠ¤íƒ)

```text
litellm/
â””â”€ docker-compose.yml
   â”œâ”€ ollama (11434) â†’ tinyllama, gpt-oss:20b (ì„ íƒì ), bge-m3
   â”œâ”€ litellm (4444)
   â””â”€ litellm_db (5432)
```

**í•µì‹¬ ê°œì„ :**

- âœ… `tinyllama1` ì œê±° â†’ `ollama`ë¡œ í†µí•© (ì¼ê´€ì„±)
- âœ… ë‹¨ì¼ Ollama ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ëª¨ë“  ëª¨ë¸ ê´€ë¦¬
- âœ… ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PCì— ë”°ë¼ ëª¨ë¸ë§Œ ì„ íƒì  ë¡œë“œ

---

## 2. ìµœì¢… ì•„í‚¤í…ì²˜

### 2.1 ì»´í¬ë„ŒíŠ¸ ë‹¤ì´ì–´ê·¸ë¨

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì‚¬ìš©ì ì• í”Œë¦¬ì¼€ì´ì…˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“ http://localhost:4444
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      LiteLLM Proxy (í¬íŠ¸ 4444)           â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ ëª¨ë¸ ë¼ìš°íŒ… í…Œì´ë¸”                 â”‚  â”‚
        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
        â”‚  â”‚ tinyllama      â†’ ollama:11434     â”‚  â”‚
        â”‚  â”‚ gpt-oss-20b    â†’ ollama:11434     â”‚  â”‚
        â”‚  â”‚ bge-m3         â†’ ollama:11434     â”‚  â”‚
        â”‚  â”‚ gemini-2.5-pro â†’ Gemini API       â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                     â”‚    â”‚              â”‚
     â†“                     â†“    â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ollama       â”‚  â”‚ litellm_db       â”‚  â”‚ Gemini API   â”‚
â”‚ (11434)      â”‚  â”‚ (5432)           â”‚  â”‚ (ì™¸ë¶€)       â”‚
â”‚              â”‚  â”‚                  â”‚  â”‚              â”‚
â”‚ â”œâ”€ tinyllama â”‚  â”‚ â”œâ”€ ëª¨ë¸ ì„¤ì •     â”‚  â”‚              â”‚
â”‚ â”œâ”€ gpt-oss   â”‚  â”‚ â””â”€ ì‚¬ìš© ë¡œê·¸     â”‚  â”‚              â”‚
â”‚ â””â”€ bge-m3    â”‚  â”‚                  â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                 â”‚
  â”‚ ëª¨ë¸ ì„œë¹™        â”‚ ìƒíƒœ ì €ì¥
  â”‚ (GPU ê°€ì†)      â”‚

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       litellm-network (Docker ë¸Œë¦¿ì§€)        â”‚
  â”‚  - ì»¨í…Œì´ë„ˆ ê°„ ì„œë¹„ìŠ¤ëª…ìœ¼ë¡œ í†µì‹               â”‚
  â”‚  - ollama, litellm, litellm_db ê³µìœ          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì„¤ê³„ ì›ì¹™

#### í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬ (Process Isolation)

- **ollama**: ëª¨ë¸ ë¡œë”©, ì¶”ë¡  ì‹¤í–‰ (GPU ì§‘ì•½ì )
- **litellm**: ë¼ìš°íŒ…, ì¸ì¦, ë¡œê¹… (CPU ì§‘ì•½ì )
- **litellm_db**: ìƒíƒœ ì €ì¥ (I/O ì§‘ì•½ì )

#### ë„¤íŠ¸ì›Œí¬ ê³µìœ  (Network Sharing)

- ë™ì¼ Docker Compose ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- ì„œë¹„ìŠ¤ëª…ìœ¼ë¡œ ì§ì ‘ í†µì‹ : `http://ollama:11434`
- ì™¸ë¶€ ë…¸ì¶œ: litellm (4444), ollama (11434) ì„ íƒì 

#### ë°ì´í„° ê²©ë¦¬ (Data Isolation)

- ê° ì„œë¹„ìŠ¤ë³„ ë…ë¦½ ë³¼ë¥¨
- `ollama_data`: ëª¨ë¸ íŒŒì¼ (~13GB)
- `postgres_data`: LiteLLM ì„¤ì • ë° ë¡œê·¸

---

## 3. êµ¬í˜„: ë‹¨ì¼ Compose ìŠ¤íƒ

### 3.1 ìµœì¢… docker-compose.yml

**ìœ„ì¹˜**: `/home/bwyoon/para/project/litellm/docker-compose.yml`

```yaml
services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Ollama: ë¡œì»¬ LLM ì¶”ë¡  ì—”ì§„ (í†µí•©)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # ëª¨ë¸ ìë™ ì–¸ë¡œë“œ ì‹œê°„ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
      OLLAMA_KEEP_ALIVE: "5m"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # PostgreSQL: LiteLLM ì„¤ì • ë° ë¡œê·¸ ì €ì¥ì†Œ
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    ports:
      - "5431:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # LiteLLM: AI Gateway (Proxy)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-v1.73.0-stable
    restart: unless-stopped
    env_file: .env
    volumes:
      - ./litellm_settings.yml:/app/config.yml
    command:
      - "--config=config.yml"
    ports:
      - "4444:4000"
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
      STORE_MODEL_IN_DB: "True"
      LITELLM_MASTER_KEY: "sk-4444"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  postgres_data:
    name: litellm_postgres_data
  ollama_data:
    name: litellm_ollama_data

networks:
  default:
    name: litellm-network
```

### 3.2 ìµœì¢… litellm_settings.yml

**ìœ„ì¹˜**: `/home/bwyoon/para/project/litellm/litellm_settings.yml`

```yaml
model_list:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Ollama ë¡œì»¬ ëª¨ë¸ (ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # ì €ì‚¬ì–‘ PC: í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ (~50MB)
  - model_name: tinyllama
    litellm_params:
      model: ollama/tinyllama
      api_base: http://ollama:11434
    model_info:
      mode: chat
      supports_function_calling: false
      max_tokens: 2048

  # ê³ ì‚¬ì–‘ PC: ì„ íƒì  ì‚¬ìš© (~11GB VRAM)
  - model_name: gpt-oss-20b
    litellm_params:
      model: ollama/gpt-oss:20b
      api_base: http://ollama:11434
    model_info:
      mode: chat
      supports_function_calling: true
      max_tokens: 8192

  # ì„ë² ë”© ëª¨ë¸ (~2GB)
  - model_name: bge-m3
    litellm_params:
      model: ollama/bge-m3:latest
      api_base: http://ollama:11434
    model_info:
      mode: embedding
      max_input_tokens: 8192

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ì™¸ë¶€ API ëª¨ë¸
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY

general:
  debug: true
  # ìš”ì²­ ë¡œê¹… í™œì„±í™”
  litellm_settings:
    success_callback: ["postgres"]
    failure_callback: ["postgres"]
```

---

## 4. ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ëŒ€ì‘

### 4.1 ëª¨ë¸ ì„ íƒ ì „ëµ

#### ì €ì‚¬ì–‘ PC (VRAM < 8GB)

**ê¶Œì¥ ëª¨ë¸:**

- âœ… `tinyllama` (50MB) - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
- âœ… `gemini-*` - ì™¸ë¶€ API (VRAM ë¶ˆí•„ìš”)

**ì´ˆê¸° ì„¤ì •:**

```bash
cd /home/bwyoon/para/project/litellm

# 1. Compose ìŠ¤íƒ ì‹œì‘
docker compose up -d

# 2. ê°€ë²¼ìš´ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull tinyllama

# 3. í…ŒìŠ¤íŠ¸
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "messages": [{"role": "user", "content": "ì•ˆë…•?"}]
  }'
```

#### ê³ ì‚¬ì–‘ PC (VRAM >= 16GB)

**ê¶Œì¥ ëª¨ë¸:**

- âœ… `tinyllama` (50MB) - í…ŒìŠ¤íŠ¸ìš©
- âœ… `gpt-oss-20b` (11GB) - ë©”ì¸ ëª¨ë¸
- âœ… `bge-m3` (2GB) - ì„ë² ë”©
- âœ… `gemini-*` - ë³´ì¡° ëª¨ë¸

**ì´ˆê¸° ì„¤ì •:**

```bash
cd /home/bwyoon/para/project/litellm

# 1. Compose ìŠ¤íƒ ì‹œì‘
docker compose up -d

# 2. ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰)
docker exec ollama ollama pull tinyllama        # ~1ë¶„
docker exec ollama ollama pull gpt-oss:20b      # ~10ë¶„ (11GB)
docker exec ollama ollama pull bge-m3:latest    # ~3ë¶„

# 3. gpt-oss:20b ì‚¬ì „ ë¡œë“œ (ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)
docker exec ollama ollama run gpt-oss:20b "ì•ˆë…•?" --verbose

# 4. í…ŒìŠ¤íŠ¸
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [{"role": "user", "content": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?"}]
  }'
```

### 4.2 ë™ì  ëª¨ë¸ ê´€ë¦¬

#### ëª¨ë¸ ëª©ë¡ í™•ì¸

```bash
# Ollamaì— ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸
docker exec ollama ollama list

# LiteLLMì— ë“±ë¡ëœ ëª¨ë¸
curl http://localhost:4444/models \
  -H "Authorization: Bearer sk-4444"
```

#### ëª¨ë¸ ì¶”ê°€/ì œê±°

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull <model-name>

# ëª¨ë¸ ì‚­ì œ (VRAM í™•ë³´)
docker exec ollama ollama rm <model-name>
```

#### VRAM ëª¨ë‹ˆí„°ë§

```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker exec ollama nvidia-smi

# Ollama ë¡œê·¸ì—ì„œ ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
docker logs ollama | grep -E "loaded|offloaded"
```

### 4.3 ìë™ ëª¨ë¸ ì„ íƒ ìŠ¤í¬ë¦½íŠ¸

**ìœ„ì¹˜**: `/home/bwyoon/para/project/litellm/setup_models.sh`

```bash
#!/bin/bash
# setup_models.sh - PC ì‚¬ì–‘ì— ë”°ë¥¸ ìë™ ëª¨ë¸ ì„¤ì •

set -e

echo "=================================="
echo "LiteLLM ëª¨ë¸ ìë™ ì„¤ì •"
echo "=================================="
echo ""

# VRAM í™•ì¸ (nvidia-smi í•„ìš”)
if command -v nvidia-smi &> /dev/null; then
    VRAM_MB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    VRAM_GB=$((VRAM_MB / 1024))
    echo "âœ“ GPU ê°ì§€: ${VRAM_GB}GB VRAM"
else
    VRAM_GB=0
    echo "âš  GPU ë¯¸ê°ì§€ (CPU ëª¨ë“œ)"
fi

echo ""

# Compose ìŠ¤íƒ ì‹œì‘
echo "[1/3] Docker Compose ìŠ¤íƒ ì‹œì‘ ì¤‘..."
docker compose up -d
sleep 5

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
echo ""
echo "[2/3] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."

# ê¸°ë³¸ ëª¨ë¸ (í•­ìƒ)
echo "  - tinyllama ë‹¤ìš´ë¡œë“œ ì¤‘ (~50MB)..."
docker exec ollama ollama pull tinyllama

# ì‚¬ì–‘ë³„ ëª¨ë¸
if [ "$VRAM_GB" -ge 16 ]; then
    echo "  âœ“ ê³ ì‚¬ì–‘ PC ê°ì§€ (${VRAM_GB}GB VRAM)"
    echo "  - gpt-oss:20b ë‹¤ìš´ë¡œë“œ ì¤‘ (~11GB, ì•½ 10ë¶„ ì†Œìš”)..."
    docker exec ollama ollama pull gpt-oss:20b

    echo "  - bge-m3 ë‹¤ìš´ë¡œë“œ ì¤‘ (~2GB)..."
    docker exec ollama ollama pull bge-m3:latest

    echo "  - gpt-oss:20b ì‚¬ì „ ë¡œë“œ ì¤‘..."
    docker exec ollama ollama run gpt-oss:20b "í…ŒìŠ¤íŠ¸" > /dev/null 2>&1 || true

    MODELS="tinyllama, gpt-oss-20b, bge-m3"
else
    echo "  âœ“ ì €ì‚¬ì–‘ PC ê°ì§€ (${VRAM_GB}GB VRAM)"
    echo "  âš  gpt-oss:20bëŠ” ìƒëµí•©ë‹ˆë‹¤ (16GB VRAM ê¶Œì¥)"
    MODELS="tinyllama"
fi

# í…ŒìŠ¤íŠ¸
echo ""
echo "[3/3] ì„¤ì • í™•ì¸ ì¤‘..."
echo ""

# LiteLLM í—¬ìŠ¤ ì²´í¬
if curl -f http://localhost:4444/health/liveliness > /dev/null 2>&1; then
    echo "âœ“ LiteLLM í”„ë¡ì‹œ ì •ìƒ"
else
    echo "âœ— LiteLLM í”„ë¡ì‹œ ì‘ë‹µ ì—†ìŒ"
    exit 1
fi

# ëª¨ë¸ ëª©ë¡ í™•ì¸
echo "âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ${MODELS}"

echo ""
echo "=================================="
echo "ì„¤ì • ì™„ë£Œ!"
echo "=================================="
echo ""
echo "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:"
echo ""
echo "  curl http://localhost:4444/v1/chat/completions \\"
echo "    -H \"Authorization: Bearer sk-4444\" \\"
echo "    -H \"Content-Type: application/json\" \\"
echo "    -d '{"
echo "      \"model\": \"tinyllama\","
echo "      \"messages\": [{\"role\": \"user\", \"content\": \"ì•ˆë…•?\"}]"
echo "    }'"
echo ""
```

**ì‚¬ìš©ë²•:**

```bash
cd /home/bwyoon/para/project/litellm
chmod +x setup_models.sh
./setup_models.sh
```

---

## 5. SOLID ì›ì¹™ ì¤€ìˆ˜ í™•ì¸

### 5.1 Single Responsibility Principle (SRP)

| ì»´í¬ë„ŒíŠ¸ | ë‹¨ì¼ ì±…ì„ | ì¤€ìˆ˜ ì—¬ë¶€ |
|----------|----------|----------|
| **ollama** | ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ì‹¤í–‰ | âœ… |
| **litellm** | API ë¼ìš°íŒ…, ì¸ì¦, ë¡œê¹… | âœ… |
| **litellm_db** | ìƒíƒœ ì €ì¥ | âœ… |

**í‰ê°€**: âœ… **ì¤€ìˆ˜**
ê° ì„œë¹„ìŠ¤ê°€ ëª…í™•í•œ ë‹¨ì¼ ì±…ì„ì„ ê°€ì§. í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬ë¡œ ë…ë¦½ì  ë°°í¬ ê°€ëŠ¥.

### 5.2 Open/Closed Principle (OCP)

**í™•ì¥ ì‹œë‚˜ë¦¬ì˜¤:**

1. **ìƒˆ Ollama ëª¨ë¸ ì¶”ê°€**
   - ë³€ê²½: `litellm_settings.yml`ë§Œ ìˆ˜ì •
   - ì½”ë“œ ë³€ê²½: ë¶ˆí•„ìš” âœ…

2. **ìƒˆ ì™¸ë¶€ API ì¶”ê°€** (ì˜ˆ: Claude)
   - ë³€ê²½: `litellm_settings.yml` + `.env`
   - ì½”ë“œ ë³€ê²½: ë¶ˆí•„ìš” âœ…

3. **Ollamaë¥¼ vLLMìœ¼ë¡œ êµì²´**
   - ë³€ê²½: `docker-compose.yml`ì˜ ollama ì„œë¹„ìŠ¤ë§Œ êµì²´
   - LiteLLM ì½”ë“œ: ë³€ê²½ ë¶ˆí•„ìš” âœ…

**í‰ê°€**: âœ… **ì¤€ìˆ˜**
ì„¤ì • ë³€ê²½ë§Œìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥. ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”.

### 5.3 Dependency Inversion Principle (DIP)

```text
ìƒìœ„ ê³„ì¸µ (High-level)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LiteLLM    â”‚ â”€â”€â”€â”€â†’ "OpenAI í˜¸í™˜ API" (ì¶”ìƒí™”)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â†‘
                              â”‚ êµ¬í˜„
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Ollama â”‚       â”‚ Gemini â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  (êµ¬í˜„ì²´)         (êµ¬í˜„ì²´)
```

**í‰ê°€**: âœ… **ì¤€ìˆ˜**
LiteLLMì€ êµ¬ì²´ì ì¸ Ollamaì— ì˜ì¡´í•˜ì§€ ì•Šê³ , "OpenAI í˜¸í™˜ API" ì¸í„°í˜ì´ìŠ¤ì— ì˜ì¡´.

### 5.4 ì¢…í•© í‰ê°€

| SOLID ì›ì¹™ | ì´ì „ (ë¶„ë¦¬) | í˜„ì¬ (í†µí•© Compose) |
|-----------|------------|---------------------|
| SRP | âœ… | âœ… |
| OCP | âš ï¸ (ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë³µì¡) | âœ… (ì„¤ì •ë§Œ ë³€ê²½) |
| LSP | âœ… | âœ… |
| ISP | âœ… | âœ… |
| DIP | âœ… | âœ… |
| **ê´€ë¦¬ ë³µì¡ë„** | âŒ ë†’ìŒ | âœ… ë‚®ìŒ |

**ê²°ë¡ **: ë‹¨ì¼ Compose ìŠ¤íƒì´ SOLID ì›ì¹™ì„ ìœ ì§€í•˜ë©´ì„œ ê´€ë¦¬ ë³µì¡ë„ë¥¼ ë‚®ì¶¤.

---

## 6. ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 6.1 ì‚¬ì „ ì¤€ë¹„

#### Step 1: ê¸°ì¡´ devnet_env_setupì˜ ëª¨ë¸ í™•ì¸

```bash
cd /home/bwyoon/para/project/devnet_env_setup/ollama_setup

# í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
docker exec ollama ollama list

# ì˜ˆìƒ ì¶œë ¥:
# NAME                  ID              SIZE      MODIFIED
# gpt-oss:20b          abc123...       11 GB     2 days ago
# bge-m3:latest        def456...       2.0 GB    2 days ago
```

#### Step 2: ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°±ì—…

```bash
# litellm í”„ë¡œì íŠ¸ ë°±ì—…
cd /home/bwyoon/para/project/litellm
cp docker-compose.yml docker-compose.yml.backup
cp litellm_settings.yml litellm_settings.yml.backup

# devnet_env_setup ì¤‘ì§€ (ëª¨ë¸ íŒŒì¼ì€ ìœ ì§€)
cd /home/bwyoon/para/project/devnet_env_setup/ollama_setup
docker compose down
# ì£¼ì˜: -v ì˜µì…˜ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ (ë³¼ë¥¨ ì‚­ì œë¨)
```

### 6.2 ìƒˆ êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

#### Step 1: litellm í”„ë¡œì íŠ¸ ì—…ë°ì´íŠ¸

```bash
cd /home/bwyoon/para/project/litellm

# 1. ê¸°ì¡´ ìŠ¤íƒ ì¤‘ì§€ ë° ì •ë¦¬
docker compose down -v

# 2. ìƒˆ docker-compose.yml ì ìš© (ìœ„ 3.1ì ˆ ì°¸ì¡°)
# - tinyllama1 ì„œë¹„ìŠ¤ ì œê±°
# - ollama ì„œë¹„ìŠ¤ ì¶”ê°€
# - litellmì˜ depends_onì— ollama ì¶”ê°€

# 3. ìƒˆ litellm_settings.yml ì ìš© (ìœ„ 3.2ì ˆ ì°¸ì¡°)
# - tinyllama1 â†’ ollamaë¡œ ë³€ê²½
# - api_baseë¥¼ http://ollama:11434ë¡œ ë³€ê²½

# 4. ìƒˆ ìŠ¤íƒ ì‹œì‘
docker compose up -d
```

#### Step 2: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

**ì €ì‚¬ì–‘ PC:**

```bash
docker exec ollama ollama pull tinyllama
```

**ê³ ì‚¬ì–‘ PC:**

```bash
# ìë™ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)
chmod +x setup_models.sh
./setup_models.sh

# ë˜ëŠ” ìˆ˜ë™ ì„¤ì •
docker exec ollama ollama pull tinyllama
docker exec ollama ollama pull gpt-oss:20b
docker exec ollama ollama pull bge-m3:latest
```

#### Step 3: ê²€ì¦

```bash
# 1. ëª¨ë“  ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬
docker compose ps

# ì˜ˆìƒ ì¶œë ¥: (ëª¨ë‘ "healthy" ìƒíƒœ)
# NAME         STATUS
# ollama       Up (healthy)
# litellm      Up (healthy)
# litellm_db   Up (healthy)

# 2. ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:4444/models \
  -H "Authorization: Bearer sk-4444" | jq '.data[].id'

# ì˜ˆìƒ ì¶œë ¥:
# "tinyllama"
# "gpt-oss-20b"
# "bge-m3"
# "gemini-2.5-pro"

# 3. ì¶”ë¡  í…ŒìŠ¤íŠ¸
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "messages": [{"role": "user", "content": "1+1=?"}]
  }' | jq '.choices[0].message.content'

# 4. GPU ì‚¬ìš© í™•ì¸ (ê³ ì‚¬ì–‘ PC)
docker exec ollama nvidia-smi
```

### 6.3 ê¸°ì¡´ devnet_env_setup í”„ë¡œì íŠ¸ ì²˜ë¦¬

#### Option A: ì™„ì „ ì œê±° (ê¶Œì¥)

```bash
cd /home/bwyoon/para/project/devnet_env_setup/ollama_setup

# ì»¨í…Œì´ë„ˆ ë° ë³¼ë¥¨ ì‚­ì œ
docker compose down -v

# í”„ë¡œì íŠ¸ ë³´ê´€ (ì‚­ì œ ì „ ë°±ì—…)
cd /home/bwyoon/para/project
mv devnet_env_setup devnet_env_setup.deprecated
```

#### Option B: ì°¸ê³ ìš©ìœ¼ë¡œ ë³´ê´€

```bash
# READMEì— "Deprecated" í‘œì‹œ ì¶”ê°€
cd /home/bwyoon/para/project/devnet_env_setup
echo "" >> README.md
echo "## âš ï¸ Deprecated" >> README.md
echo "ì´ í”„ë¡œì íŠ¸ëŠ” litellm í”„ë¡œì íŠ¸ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤." >> README.md
echo "ìƒˆ ìœ„ì¹˜: /home/bwyoon/para/project/litellm" >> README.md
```

### 6.4 ë¡¤ë°± ì ˆì°¨ (ë¬¸ì œ ë°œìƒ ì‹œ)

```bash
cd /home/bwyoon/para/project/litellm

# 1. ìƒˆ ìŠ¤íƒ ì¤‘ì§€
docker compose down

# 2. ë°±ì—… ë³µì›
cp docker-compose.yml.backup docker-compose.yml
cp litellm_settings.yml.backup litellm_settings.yml

# 3. ì´ì „ ìŠ¤íƒ ì¬ì‹œì‘
docker compose up -d

# 4. devnet_env_setup ì¬ì‹œì‘
cd /home/bwyoon/para/project/devnet_env_setup/ollama_setup
docker compose up -d
```

---

## 7. ìš´ì˜ ê°€ì´ë“œ

### 7.1 ì¼ìƒ ì‘ì—…

#### ì „ì²´ ìŠ¤íƒ ì‹œì‘/ì¢…ë£Œ

```bash
cd /home/bwyoon/para/project/litellm

# ì‹œì‘
docker compose up -d

# ì¢…ë£Œ
docker compose down

# ì „ì²´ ì¬ì‹œì‘ (ì„¤ì • ë³€ê²½ í›„)
docker compose restart

# íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¬ì‹œì‘
docker compose restart litellm
```

#### ëª¨ë¸ ê´€ë¦¬

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull <model-name>

# ëª¨ë¸ ëª©ë¡
docker exec ollama ollama list

# ëª¨ë¸ ì‚­ì œ (VRAM í™•ë³´)
docker exec ollama ollama rm <model-name>

# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)
docker exec ollama ollama run gpt-oss:20b "test"
```

#### ë¡œê·¸ í™•ì¸

```bash
# ì „ì²´ ë¡œê·¸
docker compose logs

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
docker compose logs ollama
docker compose logs litellm

# ì‹¤ì‹œê°„ ë¡œê·¸ (tail -f)
docker compose logs -f litellm

# ìµœê·¼ 50ì¤„
docker compose logs --tail=50 ollama
```

### 7.2 ëª¨ë‹ˆí„°ë§

#### í—¬ìŠ¤ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸

**ìœ„ì¹˜**: `/home/bwyoon/para/project/litellm/health_check.sh`

```bash
#!/bin/bash
# health_check.sh - ì „ì²´ ìŠ¤íƒ í—¬ìŠ¤ ì²´í¬

echo "========================================"
echo "  LiteLLM ìŠ¤íƒ í—¬ìŠ¤ ì²´í¬"
echo "========================================"
echo ""

# 1. Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ
echo "[1/4] ì»¨í…Œì´ë„ˆ ìƒíƒœ"
docker compose ps

echo ""

# 2. Ollama í—¬ìŠ¤
echo "[2/4] Ollama í—¬ìŠ¤"
if curl -f http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "  âœ“ Ollama API ì •ìƒ"
    MODELS=$(docker exec ollama ollama list | tail -n +2 | wc -l)
    echo "  âœ“ ëª¨ë¸ ê°œìˆ˜: ${MODELS}"
else
    echo "  âœ— Ollama API ì‘ë‹µ ì—†ìŒ"
fi

echo ""

# 3. LiteLLM í—¬ìŠ¤
echo "[3/4] LiteLLM í—¬ìŠ¤"
if curl -f http://localhost:4444/health/liveliness > /dev/null 2>&1; then
    echo "  âœ“ LiteLLM í”„ë¡ì‹œ ì •ìƒ"

    # ëª¨ë¸ ê°œìˆ˜
    MODEL_COUNT=$(curl -s http://localhost:4444/models \
      -H "Authorization: Bearer sk-4444" | jq '.data | length')
    echo "  âœ“ ë“±ë¡ëœ ëª¨ë¸: ${MODEL_COUNT}"
else
    echo "  âœ— LiteLLM í”„ë¡ì‹œ ì‘ë‹µ ì—†ìŒ"
fi

echo ""

# 4. GPU ìƒíƒœ (ìˆëŠ” ê²½ìš°)
echo "[4/4] GPU ìƒíƒœ"
if docker exec ollama nvidia-smi > /dev/null 2>&1; then
    docker exec ollama nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
else
    echo "  âš  GPU ë¯¸ì‚¬ìš© ë˜ëŠ” ë¯¸ê°ì§€"
fi

echo ""
echo "========================================"
echo "  í—¬ìŠ¤ ì²´í¬ ì™„ë£Œ"
echo "========================================"
```

**ì‚¬ìš©ë²•:**

```bash
chmod +x health_check.sh
./health_check.sh
```

### 7.3 ì„±ëŠ¥ ìµœì í™”

#### GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

```yaml
# docker-compose.ymlì˜ ollama ì„œë¹„ìŠ¤
environment:
  # 5ë¶„ í›„ ìë™ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í™•ë³´)
  OLLAMA_KEEP_ALIVE: "5m"

  # ë˜ëŠ” í•­ìƒ ë¡œë“œ ìœ ì§€ (ë¹ ë¥¸ ì‘ë‹µ)
  # OLLAMA_KEEP_ALIVE: "-1"
```

#### ë™ì‹œ ìš”ì²­ ì²˜ë¦¬

```yaml
# litellm_settings.yml
model_list:
  - model_name: gpt-oss-20b
    litellm_params:
      model: ollama/gpt-oss:20b
      api_base: http://ollama:11434
      num_retries: 3
      timeout: 300
    model_info:
      # ë™ì‹œ ìš”ì²­ ì œí•œ
      max_parallel_requests: 2
```

#### ëª¨ë¸ ì›Œë°ì—…

```bash
# ìŠ¤íƒ ì‹œì‘ í›„ ìë™ ì›Œë°ì—…
cd /home/bwyoon/para/project/litellm
docker compose up -d
sleep 10

# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
docker exec ollama ollama run gpt-oss:20b "warm up" > /dev/null 2>&1 &
```

---

## 8. ë¬¸ì œ í•´ê²°

### 8.1 ì¼ë°˜ì ì¸ ë¬¸ì œ

#### ë¬¸ì œ: "service 'ollama' failed to build"

**ì›ì¸**: GPU ë“œë¼ì´ë²„ ë˜ëŠ” NVIDIA Container Toolkit ë¯¸ì„¤ì¹˜

**í•´ê²°:**

```bash
# 1. GPU ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# 2. NVIDIA Container Toolkit ì„¤ì¹˜
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -fsSL https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# 3. í…ŒìŠ¤íŠ¸
docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi
```

#### ë¬¸ì œ: "Model 'gpt-oss-20b' not found"

**ì›ì¸**: Ollamaì— ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•ŠìŒ

**í•´ê²°:**

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull gpt-oss:20b

# ë‹¤ìš´ë¡œë“œ í™•ì¸
docker exec ollama ollama list
```

#### ë¬¸ì œ: "Connection refused" (<http://ollama:11434>)

**ì›ì¸**: Ollama ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ

**í•´ê²°:**

```bash
# 1. Ollama ìƒíƒœ í™•ì¸
docker compose ps ollama

# 2. Ollama ë¡œê·¸ í™•ì¸
docker compose logs ollama

# 3. Ollama ì¬ì‹œì‘
docker compose restart ollama

# 4. í—¬ìŠ¤ ì²´í¬
curl http://localhost:11434/api/tags
```

### 8.2 ì„±ëŠ¥ ë¬¸ì œ

#### ë¬¸ì œ: ì²« ìš”ì²­ì´ ë§¤ìš° ëŠë¦¼ (30ì´ˆ+)

**ì›ì¸**: ëª¨ë¸ ë¡œë”© ì‹œê°„ (Cold Start)

**í•´ê²°:**

```bash
# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì›Œë°ì—…)
docker exec ollama ollama run gpt-oss:20b "warm up"

# ë˜ëŠ” OLLAMA_KEEP_ALIVE ì„¤ì • ë³€ê²½
# docker-compose.yml:
#   environment:
#     OLLAMA_KEEP_ALIVE: "-1"  # í•­ìƒ ë©”ëª¨ë¦¬ì— ìœ ì§€
```

#### ë¬¸ì œ: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (CUDA out of memory)

**ì›ì¸**: ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— ë¡œë“œí•˜ë ¤ í•¨

**í•´ê²°:**

```bash
# 1. ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì–¸ë¡œë“œ
docker exec ollama ollama stop gpt-oss:20b

# 2. OLLAMA_KEEP_ALIVE ì‹œê°„ ë‹¨ì¶•
# docker-compose.yml:
#   environment:
#     OLLAMA_KEEP_ALIVE: "2m"  # 2ë¶„ í›„ ìë™ ì–¸ë¡œë“œ

# 3. ê²½ëŸ‰ ëª¨ë¸ë§Œ ì‚¬ìš©
docker exec ollama ollama rm gpt-oss:20b
# tinyllamaë§Œ ì‚¬ìš©
```

### 8.3 ë””ë²„ê¹…

#### LiteLLM ë””ë²„ê·¸ ëª¨ë“œ

```yaml
# litellm_settings.yml
general:
  debug: true  # ìƒì„¸ ë¡œê·¸ í™œì„±í™”
```

#### Ollama ìƒì„¸ ë¡œê·¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
docker compose logs -f ollama | grep -E "loaded|offloaded|error"
```

#### ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸

```bash
# litellm ì»¨í…Œì´ë„ˆì—ì„œ ollama ì ‘ê·¼ í™•ì¸
docker exec litellm curl http://ollama:11434/api/tags

# ì˜ˆìƒ ì¶œë ¥: {"models": [...]}
```

---

## 9. ë‹¤ìŒ ë‹¨ê³„

### 9.1 ì¶”ê°€ ê°œì„  ì‚¬í•­

1. **CI/CD íŒŒì´í”„ë¼ì¸**
   - GitHub Actionsë¡œ ìë™ ë°°í¬
   - ì„¤ì • ë³€ê²½ ì‹œ ìë™ í…ŒìŠ¤íŠ¸

2. **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
   - Prometheus + Grafana í†µí•©
   - ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰ ì¶”ì 

3. **ë³´ì•ˆ ê°•í™”**
   - API í‚¤ ì™¸ë¶€í™” (Vault, AWS Secrets Manager)
   - HTTPS ì ìš© (Nginx + Let's Encrypt)

4. **ë©€í‹° GPU ì§€ì›**
   - ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œë°¸ëŸ°ì‹±
   - ëª¨ë¸ë³„ GPU í• ë‹¹

### 9.2 ì°¸ê³  ìë£Œ

- [LiteLLM ê³µì‹ ë¬¸ì„œ](https://docs.litellm.ai/)
- [Ollama ê³µì‹ ë¬¸ì„œ](https://github.com/ollama/ollama)
- [Docker Compose ë„¤íŠ¸ì›Œí‚¹](https://docs.docker.com/compose/networking/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ ê²°ì •ì‚¬í•­

| í•­ëª© | ê²°ì • |
|------|------|
| **ì•„í‚¤í…ì²˜** | ë‹¨ì¼ Compose ìŠ¤íƒ + í”„ë¡œì„¸ìŠ¤ ë¶„ë¦¬ |
| **Ollama í†µí•©** | tinyllama1 ì œê±° â†’ ollamaë¡œ í†µí•© |
| **ë„¤íŠ¸ì›Œí¬** | Docker Compose ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ (ì„œë¹„ìŠ¤ëª… í†µì‹ ) |
| **ëª¨ë¸ ê´€ë¦¬** | ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ìë™ ì„ íƒ ì§€ì› |
| **SOLID ì¤€ìˆ˜** | âœ… ëª¨ë“  ì›ì¹™ ì¤€ìˆ˜ |

### 10.2 ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°±ì—…
- [ ] devnet_env_setup ì¤‘ì§€
- [ ] litellm í”„ë¡œì íŠ¸ ì—…ë°ì´íŠ¸ (docker-compose.yml, litellm_settings.yml)
- [ ] ìƒˆ ìŠ¤íƒ ì‹œì‘
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (setup_models.sh ë˜ëŠ” ìˆ˜ë™)
- [ ] ê²€ì¦ (ëª¨ë¸ ëª©ë¡, ì¶”ë¡  í…ŒìŠ¤íŠ¸)
- [ ] devnet_env_setup í”„ë¡œì íŠ¸ ì²˜ë¦¬ (ì œê±° ë˜ëŠ” ë³´ê´€)

### 10.3 ì¦‰ì‹œ ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# 1. í”„ë¡œì íŠ¸ë¡œ ì´ë™
cd /home/bwyoon/para/project/litellm

# 2. ê¸°ì¡´ ë°±ì—…
cp docker-compose.yml docker-compose.yml.backup
cp litellm_settings.yml litellm_settings.yml.backup

# 3. ìƒˆ ì„¤ì • ì ìš© (ìœ„ 3.1, 3.2ì ˆ ì°¸ì¡°)

# 4. ê¸°ì¡´ ìŠ¤íƒ ì¤‘ì§€
docker compose down -v

# 5. ìƒˆ ìŠ¤íƒ ì‹œì‘
docker compose up -d

# 6. ìë™ ëª¨ë¸ ì„¤ì •
chmod +x setup_models.sh
./setup_models.sh

# 7. í—¬ìŠ¤ ì²´í¬
chmod +x health_check.sh
./health_check.sh
```

---

**ì‘ì„±ì**: Claude Sonnet 4.5
**ë²„ì „**: Final v1.0
**ìµœì¢… ìˆ˜ì •**: 2025-12-08

**Happy LLM Serving! ğŸš€**
