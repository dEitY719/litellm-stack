# LiteLLM Stack

Unified Docker Compose stack to run **Ollama** (local LLM inference) + **LiteLLM** (OpenAI-compatible API gateway) + **PostgreSQL** together.

## Overview

- **Single `docker-compose.yml`** orchestrates all services
- **OpenAI-style API** â€” use standard OpenAI client libraries
- **Route to multiple models** â€” local Ollama models + external APIs (Gemini, OpenAI, Claude)
- **Ready-to-run examples** â€” Python scripts using LangChain and OpenAI SDK
- **Auto-setup** â€” scripts detect your hardware and configure models automatically

## Quick Start

### Prerequisites

- Docker & Docker Compose v2+
- 8GB+ RAM recommended
- (Optional) NVIDIA GPU for faster inference
- **Internal PC Users**: CA ì¸ì¦ì„œ(samsungsemi-prx.com.crt) í•„ìš” â†’ [ìƒì„¸ ê°€ì´ë“œ](docs/INTERNAL_SETUP.md)

### 1. Clone & Initialize

```bash
git clone https://github.com/dev-team-404/litellm-stack
cd litellm-stack

# ğŸ”§ Interactive initialization (í™˜ê²½ ì„ íƒ)
make init
```

**ì„ íƒ ì˜µì…˜:**
```
ğŸ  Home (ê°œì¸ PC):         ë¡œì»¬ ê°œë°œ, SSL ê²€ì¦ í™œì„±í™”
ğŸŒ External (íšŒì‚¬ ì™¸ë¶€):   ê³µê°œ GitHub, SSL ê²€ì¦ í™œì„±í™”
ğŸ¢ Internal (íšŒì‚¬ ë‚´ë¶€):   í”„ë¡ì‹œ, CA ì¸ì¦ì„œ í•„ìˆ˜
```

> **Internal PC ì‚¬ìš©ì**: [docs/INTERNAL_SETUP.md](docs/INTERNAL_SETUP.md)ì—ì„œ CA ì¸ì¦ì„œ ë‹¤ìš´ë¡œë“œ ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

### 2. Start Services

```bash
make up
```

This starts:

- **Ollama** (port 11434) â€” local LLM serving with GPU acceleration
- **LiteLLM** (port 4444) â€” OpenAI-compatible proxy
- **PostgreSQL** (port 5431) â€” stores config and usage logs

### 3. Verify It Works

```bash
# Check service status
docker compose ps

# List available models
curl http://localhost:4444/models \
  -H "Authorization: Bearer sk-4444"
```

### 4. Test a Model

```bash
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "messages": [{"role": "user", "content": "What is 2+2?"}]
  }'
```

## Model Setup

Models are auto-configured on first run. The setup script detects your hardware:

**Low-spec** (< 8GB VRAM):

- `tinyllama` â€” ~50MB, fast inference

**High-spec** (8GB+ VRAM):

- `tinyllama` â€” fast for testing
- `gpt-oss:20b` â€” powerful 20B parameter model (~11GB VRAM)
- `bge-m3` â€” embedding model for search tasks

To manually run setup:

```bash
# Auto-detect hardware and download models
make setup-models

# Or manually pull a model
docker exec ollama ollama pull gpt-oss:20b
```

## Python Examples

```bash
# Install dependencies
pip install -r requirements.txt

# Run examples
python example/test_openai.py           # OpenAI SDK style
python example/test_langchain_openai.py # LangChain
python example/test_langchain_agent.py  # Agent with tools
```

All examples use `http://localhost:4444` and key `sk-4444` (edit if you changed them).

## Configuration

### Add External API Models

Edit `.env` to add API keys:

```bash
GEMINI_API_KEY=your-key-here
OPENAI_API_KEY=your-key-here
```

Then edit `litellm_settings.yml` to add routes. Restart LiteLLM:

```bash
docker compose restart litellm
```

### GPU Optimization

Default settings in `docker-compose.yml` work for RTX 5070 Ti (16GB). For other GPUs, optionally override in `.env` (see `.env.example` for options).

## Useful Commands

```bash
make up           # Start services
make down         # Stop services
make restart      # Restart services
make logs         # Tail service logs
make health       # Check all services
```

## Troubleshooting

**Containers won't start?**
```bash
# Check logs
docker compose logs litellm

# Full restart
docker compose down -v
docker compose up -d
```

**GPU not detected?**
```bash
# On host
nvidia-smi

# In Docker
docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi
```

**Out of memory?**
```bash
# Remove unused models
docker exec ollama ollama rm model-name

# Check GPU usage
docker exec ollama nvidia-smi
```

## Architecture

- **Ollama**: Runs local models with GPU acceleration (port 11434)
- **LiteLLM**: Proxy server routes requests to Ollama or external APIs (port 4444)
- **PostgreSQL**: Stores API keys, usage stats, and config (port 5431)

All services communicate via internal Docker network. External clients connect to LiteLLM on port 4444.

## Contributing

Contributions welcome! Please:

1. Test your changes: `docker compose up -d && make health`
2. Follow shell/Python style guides (see CONTRIBUTING.md)
3. Update relevant docs
4. Submit a PR with clear description

## License

MIT â€” see LICENSE file for details.

## Support

- Ollama docs: https://github.com/ollama/ollama
- LiteLLM docs: https://docs.litellm.ai/
- Issues: Create a GitHub issue
