# LiteLLM Stack

> ë¡œì»¬ LLM + AI Gateway í†µí•© ìŠ¤íƒ

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](https://docs.docker.com/compose/)
[![Ollama](https://img.shields.io/badge/Ollama-GPU-green.svg)](https://github.com/ollama/ollama)

## ê°œìš”

Ollama (ë¡œì»¬ LLM)ì™€ LiteLLM (AI Gateway)ì„ ë‹¨ì¼ Docker Compose ìŠ¤íƒìœ¼ë¡œ í†µí•©í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

- **Ollama**: gpt-oss:20b, tinyllama, bge-m3 ë“± ë¡œì»¬ ëª¨ë¸ ì„œë¹™ (GPU ê°€ì† ì§€ì›)
- **LiteLLM**: í†µí•© API Gateway (Ollama + Gemini + OpenAI + ...)
- **PostgreSQL**: LiteLLM ì„¤ì • ë° ë¡œê·¸ ì €ì¥ì†Œ

### ì•„í‚¤í…ì²˜

```text
ì‚¬ìš©ì ì• í”Œë¦¬ì¼€ì´ì…˜
        â†“
   LiteLLM Proxy (4444)
     /    |    \
    /     |     \
Ollama  Gemini  OpenAI
(11434)  (API)   (API)
```

## ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Docker ë° Docker Compose v2+
- NVIDIA GPU + ë“œë¼ì´ë²„ (ì„ íƒì , GPU ê°€ì†ìš©)
- ìµœì†Œ 8GB RAM (16GB ê¶Œì¥)

### ì„¤ì¹˜

```bash
# 1. Clone
git clone https://github.com/dEitY719/litellm-stack
cd litellm-stack

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì—ì„œ GEMINI_API_KEY ë“± ì„¤ì •

# 3. ìŠ¤íƒ ì‹œì‘
docker compose up -d

# 4. ëª¨ë¸ ìë™ ì„¤ì • (ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ìë™ ê°ì§€)
chmod +x scripts/setup_models.sh
./scripts/setup_models.sh

# 5. í—¬ìŠ¤ ì²´í¬
chmod +x scripts/health_check.sh
./scripts/health_check.sh
```

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

#### 1ï¸âƒ£ curlë¡œ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•¨)

```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:4444/models \
  -H "Authorization: Bearer sk-4444"

# tinyllama í…ŒìŠ¤íŠ¸ (ì €ì‚¬ì–‘ PC)
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "messages": [{"role": "user", "content": "ì•ˆë…•?"}]
  }'

# gpt-oss:20b í…ŒìŠ¤íŠ¸ (ê³ ì‚¬ì–‘ PC, 16GB VRAM)
curl http://localhost:4444/v1/chat/completions \
  -H "Authorization: Bearer sk-4444" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [{"role": "user", "content": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?"}]
  }'
```

#### 2ï¸âƒ£ Pythonìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ê¶Œì¥)

**ì‚¬ì „ ìš”êµ¬ì‚¬í•­:**

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install openai langchain langchain-openai python-dotenv

# ë˜ëŠ” requirements.txtì—ì„œ ì„¤ì¹˜
pip install -r requirements.txt
```

**Test 1: OpenAI SDK ì§ì ‘ ì‚¬ìš©**

```bash
cd example
python test_openai.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
I'm doing wellâ€”thanks for asking! How can I help you today?
```

**Test 2: LangChain ì‚¬ìš© (Streaming + Chat)**

```bash
cd example
python test_langchain_openai.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
## í•œêµ­ì˜ 4ê³„ì ˆ(å­£ç¯€) ê°œìš”

í•œêµ­ì€ **ê³ ë¦¬ì„± ëŒ€ë¥™ì„± ê¸°í›„**ì™€ **í•´ì–‘ì„± ê¸°í›„**ê°€ ë³µí•©ì ìœ¼ë¡œ ì„ì—¬ ìˆëŠ” ê³³ì´ê¸° ë•Œë¬¸ì—,
4ê³„ì ˆì´ ëšœë ·í•˜ì§€ë§Œ ì§€ì—­ì— ë”°ë¼ ê¸°ì˜¨Â·ê°•ìˆ˜ëŸ‰ ì°¨ì´ê°€ í½ë‹ˆë‹¤. ...

AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', response_metadata={...})
```

**Python ì˜ˆì œ íŒŒì¼ ìœ„ì¹˜:**

```text
example/
â”œâ”€â”€ test_openai.py             # OpenAI SDK ì˜ˆì œ
â””â”€â”€ test_langchain_openai.py   # LangChain ì˜ˆì œ
â””â”€â”€ test_langchain_agent.py    # Agent ì˜ˆì œ
```

**ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:**

- LiteLLM API URL: `http://localhost:4444`
- API Key: `sk-4444` (ê¸°ë³¸ê°’)
- ëª¨ë¸ëª…: `gpt-oss-20b`, `tinyllama`, `bge-m3` ë“±
- ê¸°ë³¸ í—¤ë” ì„¤ì •ì€ ì˜ˆì œ íŒŒì¼ì—ì„œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

#### 3ï¸âƒ£ í†µí•© ì„¤ì • ê²€ì¦ (ì „ì²´ ì‹œìŠ¤í…œ í™•ì¸)

```bash
./scripts/test_setup.sh
```

**ê²€ì¦ í•­ëª©:**

```
[1/5] Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
  âœ“ LiteLLM ì„œë¹„ìŠ¤ ì‹¤í–‰ ì¤‘
  âœ“ Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ ì¤‘

[2/5] LiteLLM í”„ë¡ì‹œ í—¬ìŠ¤ì²´í¬
  âœ“ LiteLLM í”„ë¡ì‹œ ì‘ë‹µ ì •ìƒ

[3/5] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
  âœ“ Gemini ëª¨ë¸ ë“±ë¡ë¨
  âœ“ Ollama ë¡œì»¬ ëª¨ë¸ ë“±ë¡ë¨

[4/5] Python íŒ¨í‚¤ì§€ í™•ì¸
  âœ“ langchain ì„¤ì¹˜ë¨
  âœ“ langchain-community ì„¤ì¹˜ë¨
  âœ“ litellm ì„¤ì¹˜ë¨

[5/5] LiteLLM API ìš”ì²­ í…ŒìŠ¤íŠ¸
  âœ“ API ìš”ì²­ ì„±ê³µ

âœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!
```

#### 4ï¸âƒ£ LangChain Agent ì‹¤í–‰ (AI ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸)

**ì¤€ë¹„:**

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# ë˜ëŠ” uv ì‚¬ìš©
uv sync
```

**ì‹¤í–‰:**

```bash
python example/test_langchain_agent.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Agent initialized with Ollama backend
Agent: "Let me analyze this information..."

Response: "Here's my analysis of the Korean cultural context..."

Tool usage: Retrieved context from knowledge base
Final answer: "Based on the information provided..."
```

**LangChain Agent íŠ¹ì§•:**

- ğŸ¤– **ìë™ ì¶”ë¡ **: ë³µì¡í•œ ì‘ì—…ì„ ë‹¨ê³„ë³„ë¡œ í•´ê²°
- ğŸ”— **Tool í˜¸ì¶œ**: í•„ìš”ì‹œ ìë™ìœ¼ë¡œ ì™¸ë¶€ API/ë„êµ¬ ì‚¬ìš©
- ğŸ’­ **Chain-of-thought**: ì¶”ë¡  ê³¼ì •ì„ ì„¤ëª…í•˜ë©° ì§„í–‰
- ğŸ“Š **ìƒíƒœ ê´€ë¦¬**: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€

**í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš°:**

```bash
# 1ë‹¨ê³„: ì „ì²´ ì‹œìŠ¤í…œ ì„¤ì • í™•ì¸
./scripts/test_setup.sh

# 2ë‹¨ê³„: ê°„ë‹¨í•œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
cd example
python test_openai.py          # OpenAI SDK ì§ì ‘ ì‚¬ìš©
python test_langchain_openai.py # LangChain ê¸°ë³¸ ì‚¬ìš©

# 3ë‹¨ê³„: LangChain Agent í…ŒìŠ¤íŠ¸
python test_langchain_agent.py  # ë³µì¡í•œ ì¶”ë¡  ì‘ì—…

# 4ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
make health                      # ìƒì„¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```

## ì£¼ìš” ê¸°ëŠ¥

### ë¡œì»¬ LLM ì„œë¹™

- âœ… **GPU ê°€ì† ì§€ì›** (NVIDIA CUDA)
- âœ… **ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ê´€ë¦¬** (tinyllama, gpt-oss:20b, bge-m3)
- âœ… **ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬** (OLLAMA_KEEP_ALIVE)
- âœ… **ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ìë™ ê°ì§€**

### AI Gateway

- âœ… **OpenAI í˜¸í™˜ API** (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš© ê°€ëŠ¥)
- âœ… **ì—¬ëŸ¬ LLM í†µí•©** (Ollama, Gemini, OpenAI, Claude ë“±)
- âœ… **ë¼ìš°íŒ… ë° ë¡œë“œë°¸ëŸ°ì‹±**
- âœ… **ì¸ì¦ ë° ë¡œê¹…** (PostgreSQL)
- âœ… **ë¹„ìš© ì¶”ì ** (ì™¸ë¶€ API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§)

### ìš´ì˜ í¸ì˜ì„±

- âœ… **ë‹¨ì¼ docker-compose.yml**ë¡œ ì „ì²´ ìŠ¤íƒ ê´€ë¦¬
- âœ… **ìë™ í—¬ìŠ¤ ì²´í¬** (ìŠ¤í¬ë¦½íŠ¸ ì œê³µ)
- âœ… **í†µí•© ë¡œê¹…** (ëª¨ë“  ì»´í¬ë„ŒíŠ¸)
- âœ… **ë°ì´í„° ì˜ì†ì„±** (Docker ë³¼ë¥¨)

## ì‚¬ìš©ë²•

### Makefile ëª…ë ¹ì–´

```bash
# ì´ˆê¸° ì„¤ì •
make init              # .env íŒŒì¼ + Volume ì´ˆê¸°í™”

# Docker ê´€ë¦¬
make up                # ì „ì²´ ìŠ¤íƒ ì‹œì‘
make down              # ì „ì²´ ìŠ¤íƒ ì¢…ë£Œ
make restart           # ì¬ì‹œì‘
make rebuild           # clean + up

# ëª¨ë¸ ê´€ë¦¬
make setup-models      # ëª¨ë¸ ìë™ ì„¤ì • (GPU ê°ì§€)

# ë¡œê¹… & ëª¨ë‹ˆí„°ë§
make logs              # ì „ì²´ ë¡œê·¸
make logs-follow       # ì‹¤ì‹œê°„ ë¡œê·¸
make ps                # ì»¨í…Œì´ë„ˆ ìƒíƒœ
make health            # í—¬ìŠ¤ ì²´í¬ (ìƒì„¸ ì •ë³´)

# ì»¨í…Œì´ë„ˆ ì ‘ì†
make shell             # LiteLLM ì…¸ ì ‘ì†
make shell-db          # Database ì…¸ ì ‘ì†
make shell-ollama      # Ollama ì…¸ ì ‘ì†

# ì •ë¦¬
make clean             # ìºì‹œ ì •ë¦¬

# ë„ì›€ë§
make help              # ëª¨ë“  ëª…ë ¹ì–´ í™•ì¸
```

### ëª¨ë¸ ê´€ë¦¬

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec ollama ollama pull <model-name>

# ëª¨ë¸ ëª©ë¡
docker exec ollama ollama list

# ëª¨ë¸ ì‚­ì œ (VRAM í™•ë³´)
docker exec ollama ollama rm <model-name>

# GPU ë©”ëª¨ë¦¬ í™•ì¸
docker exec ollama nvidia-smi
```

### ì„¤ì • ë³€ê²½

#### ìƒˆ Ollama ëª¨ë¸ ì¶”ê°€

`litellm_settings.yml`ì„ ìˆ˜ì •:

```yaml
model_list:
  - model_name: llama-3-8b
    litellm_params:
      model: ollama/llama-3-8b
      api_base: http://ollama:11434
```

ê·¸ ë‹¤ìŒ ì¬ì‹œì‘:

```bash
docker compose restart litellm
```

#### ì™¸ë¶€ API ì¶”ê°€ (Gemini, OpenAI ë“±)

1. `.env`ì— API í‚¤ ì¶”ê°€:

   ```bash
   GEMINI_API_KEY=your-api-key
   ```

2. `litellm_settings.yml`ì— ëª¨ë¸ ì¶”ê°€:

   ```yaml
   - model_name: gemini-2.5-pro
     litellm_params:
       model: gemini/gemini-2.5-pro
       api_key: os.environ/GEMINI_API_KEY
   ```

3. ì¬ì‹œì‘:

   ```bash
   docker compose up -d --force-recreate litellm
   ```

## ë¬¸ì„œ

ìì„¸í•œ ë¬¸ì„œëŠ” `docs/` ë””ë ‰í† ë¦¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:

- [ì•„í‚¤í…ì²˜ ì„¤ê³„](docs/architecture-litellm-ollama-final.md) - ì „ì²´ ì•„í‚¤í…ì²˜ ë° ì„¤ê³„ ê²°ì •
- [Git Repository ì „ëµ](docs/git-repository-strategy.md) - Monorepo ê´€ë¦¬ ë°©ë²•

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ì €ì‚¬ì–‘ PC (ìµœì†Œ)

- **CPU**: 4ì½”ì–´ ì´ìƒ
- **RAM**: 8GB
- **GPU**: ë¶ˆí•„ìš” (CPU ëª¨ë“œ)
- **ë””ìŠ¤í¬**: 10GB

**ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:**

- tinyllama (~50MB)
- ì™¸ë¶€ API (Gemini, OpenAI ë“±)

### ê³ ì‚¬ì–‘ PC (ê¶Œì¥)

- **CPU**: 8ì½”ì–´ ì´ìƒ
- **RAM**: 16GB ì´ìƒ
- **GPU**: NVIDIA GPU 16GB+ VRAM
- **ë””ìŠ¤í¬**: 50GB

**ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:**

- tinyllama (~50MB)
- gpt-oss:20b (~11GB)
- bge-m3 (~2GB)
- ì™¸ë¶€ API (Gemini, OpenAI ë“±)

## ë¬¸ì œ í•´ê²°

### GPUê°€ ì¸ì‹ë˜ì§€ ì•ŠìŒ

```bash
# 1. NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# 2. Dockerì—ì„œ GPU í…ŒìŠ¤íŠ¸
docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi

# 3. NVIDIA Container Toolkit ì„¤ì¹˜ (ë¯¸ì„¤ì¹˜ ì‹œ)
# docs/architecture-litellm-ollama-final.mdì˜ 8.1ì ˆ ì°¸ì¡°
```

### ëª¨ë¸ì´ ëŠë¦¬ê²Œ ì‘ë‹µí•¨

```bash
# ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (Cold Start ë°©ì§€)
docker exec ollama ollama run gpt-oss:20b "warm up"

# ë˜ëŠ” docker-compose.ymlì—ì„œ OLLAMA_KEEP_ALIVE="-1" ì„¤ì •
```

### LiteLLMì´ ì‘ë‹µí•˜ì§€ ì•ŠìŒ

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:4444/health/liveliness

# ë¡œê·¸ í™•ì¸
docker compose logs litellm

# ì¬ì‹œì‘
docker compose restart litellm
```

ë” ìì„¸í•œ ë¬¸ì œ í•´ê²°ì€ [ì•„í‚¤í…ì²˜ ë¬¸ì„œ](docs/architecture-litellm-ollama-final.md)ì˜ 8ì¥ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ê°œë°œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
litellm-stack/
â”œâ”€â”€ docker-compose.yml              # ì „ì²´ ìŠ¤íƒ ì •ì˜
â”œâ”€â”€ litellm_settings.yml            # LiteLLM ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ .env.example                    # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ Makefile                        # í¸ì˜ ëª…ë ¹ì–´ (make help ì°¸ì¡°)
â”‚
â”œâ”€â”€ scripts/                        # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup_models.sh             # ëª¨ë¸ ìë™ ì„¤ì • (GPU ê°ì§€)
â”‚   â”œâ”€â”€ health_check.sh             # ìŠ¤íƒ í—¬ìŠ¤ ì²´í¬
â”‚   â”œâ”€â”€ list_models.sh              # LiteLLM ëª¨ë¸ ëª©ë¡
â”‚   â””â”€â”€ migrate.sh                  # Volume ë§ˆì´ê·¸ë ˆì´ì…˜
â”‚
â”œâ”€â”€ example/                        # Python í…ŒìŠ¤íŠ¸ ì˜ˆì œ
â”‚   â”œâ”€â”€ test_openai.py              # OpenAI SDK ì˜ˆì œ
â”‚   â””â”€â”€ test_langchain_openai.py    # LangChain ì˜ˆì œ
â”‚
â”œâ”€â”€ docs/                           # ìƒì„¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ architecture-litellm-ollama-final.md
â”‚   â””â”€â”€ git-repository-strategy.md
â”‚
â”œâ”€â”€ src/                            # Python í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ
â”‚   â””â”€â”€ run_langchain_agent.py      # LangChain Agent ì˜ˆì œ
â”‚
â””â”€â”€ tests/                          # í†µí•© í…ŒìŠ¤íŠ¸
```

### ê¸°ì—¬ ë°©ë²•

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ë²„ì „ ê´€ë¦¬

ì´ í”„ë¡œì íŠ¸ëŠ” [Semantic Versioning](https://semver.org/)ì„ ë”°ë¦…ë‹ˆë‹¤.

- **Major (v2.0.0)**: Breaking changes (API ë³€ê²½, êµ¬ì¡° ë³€ê²½)
- **Minor (v1.1.0)**: ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ (ìƒˆ ëª¨ë¸, ìƒˆ API ë“±)
- **Patch (v1.0.1)**: ë²„ê·¸ ìˆ˜ì •, ì„¤ì • ì¡°ì •

### ìµœì‹  ë¦´ë¦¬ìŠ¤

- **v1.0.0** (2025-12-08): ì²« ì•ˆì • ë¦´ë¦¬ìŠ¤
  - Ollama + LiteLLM í†µí•©
  - ì €ì‚¬ì–‘/ê³ ì‚¬ì–‘ PC ìë™ ê°ì§€
  - gpt-oss:20b, tinyllama, bge-m3 ì§€ì›

## ë¼ì´ì„ ìŠ¤

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ê°ì‚¬ì˜ ë§

- [Ollama](https://github.com/ollama/ollama) - ë¡œì»¬ LLM ì¶”ë¡  ì—”ì§„
- [LiteLLM](https://github.com/BerriAI/litellm) - AI Gateway
- [Docker](https://www.docker.com/) - ì»¨í…Œì´ë„ˆ í”Œë«í¼

## ì§€ì›

- **Issues**: [GitHub Issues](https://github.com/dEitY719/litellm-stack/issues)
- **Discussions**: [GitHub Discussions](https://github.com/dEitY719/litellm-stack/discussions)

---

**Made with â¤ï¸ by the community**
